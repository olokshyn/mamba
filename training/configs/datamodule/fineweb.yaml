_target_: src.datamodules.language_modeling_hf.LMDataModule
dataset_name: HuggingFaceFW/fineweb
dataset_config_name: sample-10BT
tokenizer_name: EleutherAI/gpt-neox-20b
cache_dir: ${oc.env:DATA_DIR,${data_dir}}/fineweb/cache
max_length: 2048
add_eos: True
batch_size: 16  # per GPU
batch_size_eval: ${eval:${.batch_size} * 2}
num_workers: 200  # For preprocessing only
use_shmem: False
shuffle: True
pin_memory: True
__train_len: ${div_up:374337375694, ${.max_length}}
